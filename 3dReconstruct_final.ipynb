{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrankChang28/3D_Reconstruction/blob/main/3dReconstruct_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCyoXqBGJ9qR",
        "outputId": "16a687fc-aa6e-4220-f3d8-04bbd1dad029"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "TEST_DIR = \"./test\"\n",
        "BONUS_DIR = \"./bonus\"\n",
        "SCENES_ROOT=\"./drive/MyDrive/7SCENES\"\n",
        "pointcloud_root_train=\"./drive/MyDrive/ground_truth/train_truth\"\n",
        "\n",
        "#if just generate predict data than no need these two GT point cloud\n",
        "pointcloud_root_test=\"./drive/MyDrive/ground_truth/ground_truth_data/test\"\n",
        "pointcloud_root_bonus=\"./drive/MyDrive/ground_truth/ground_truth_data/bonus\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VHQdrDtBwkqY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b0b8c9a7-bdf9-4668-97a2-c17543eccde9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pillow-heif\n",
            "  Downloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: pillow>=10.1.0 in /usr/local/lib/python3.11/dist-packages (from pillow-heif) (11.2.1)\n",
            "Downloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m6.2/7.8 MB\u001b[0m \u001b[31m185.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m113.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pillow-heif\n",
            "Successfully installed pillow-heif-0.22.0\n",
            "Collecting open3d\n",
            "  Downloading open3d-0.19.0-cp311-cp311-manylinux_2_31_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (2.0.2)\n",
            "Collecting dash>=2.6.0 (from open3d)\n",
            "  Downloading dash-3.0.4-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: werkzeug>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (3.1.3)\n",
            "Requirement already satisfied: flask>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (3.1.1)\n",
            "Requirement already satisfied: nbformat>=5.7.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (5.10.4)\n",
            "Collecting configargparse (from open3d)\n",
            "  Downloading configargparse-1.7.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting ipywidgets>=8.0.4 (from open3d)\n",
            "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting addict (from open3d)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: pillow>=9.3.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (11.2.1)\n",
            "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.11/dist-packages (from open3d) (3.10.0)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (2.2.2)\n",
            "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from open3d) (6.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.11/dist-packages (from open3d) (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from open3d) (4.67.1)\n",
            "Collecting pyquaternion (from open3d)\n",
            "  Downloading pyquaternion-0.9.9-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting flask>=3.0.0 (from open3d)\n",
            "  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting werkzeug>=3.0.0 (from open3d)\n",
            "  Downloading werkzeug-3.0.6-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (5.24.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (8.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (4.14.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (2.32.3)\n",
            "Collecting retrying (from dash>=2.6.0->open3d)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (1.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (75.2.0)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (8.2.1)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (1.9.0)\n",
            "Collecting comm>=0.1.3 (from ipywidgets>=8.0.4->open3d)\n",
            "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->open3d) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->open3d) (5.7.1)\n",
            "Collecting widgetsnbextension~=4.0.14 (from ipywidgets>=8.0.4->open3d)\n",
            "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->open3d) (3.0.15)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (2.9.0.post0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7.0->open3d) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7.0->open3d) (4.24.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7.0->open3d) (5.8.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->open3d) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->open3d) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21->open3d) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21->open3d) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21->open3d) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=3.0.0->open3d) (3.0.2)\n",
            "Collecting jedi>=0.16 (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (2.19.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.9.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.25.1)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat>=5.7.0->open3d) (4.3.8)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=5.0.0->dash>=2.6.0->open3d) (9.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3->open3d) (1.17.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->dash>=2.6.0->open3d) (3.22.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (2025.4.26)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.13)\n",
            "Downloading open3d-0.19.0-cp311-cp311-manylinux_2_31_x86_64.whl (447.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.7/447.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dash-3.0.4-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m134.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flask-3.0.3-py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.0/228.0 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading configargparse-1.7.1-py3-none-any.whl (25 kB)\n",
            "Downloading pyquaternion-0.9.9-py3-none-any.whl (14 kB)\n",
            "Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: addict, widgetsnbextension, werkzeug, retrying, pyquaternion, jedi, configargparse, comm, flask, ipywidgets, dash, open3d\n",
            "  Attempting uninstall: widgetsnbextension\n",
            "    Found existing installation: widgetsnbextension 3.6.10\n",
            "    Uninstalling widgetsnbextension-3.6.10:\n",
            "      Successfully uninstalled widgetsnbextension-3.6.10\n",
            "  Attempting uninstall: werkzeug\n",
            "    Found existing installation: Werkzeug 3.1.3\n",
            "    Uninstalling Werkzeug-3.1.3:\n",
            "      Successfully uninstalled Werkzeug-3.1.3\n",
            "  Attempting uninstall: flask\n",
            "    Found existing installation: Flask 3.1.1\n",
            "    Uninstalling Flask-3.1.1:\n",
            "      Successfully uninstalled Flask-3.1.1\n",
            "  Attempting uninstall: ipywidgets\n",
            "    Found existing installation: ipywidgets 7.7.1\n",
            "    Uninstalling ipywidgets-7.7.1:\n",
            "      Successfully uninstalled ipywidgets-7.7.1\n",
            "Successfully installed addict-2.4.0 comm-0.2.2 configargparse-1.7.1 dash-3.0.4 flask-3.0.3 ipywidgets-8.1.7 jedi-0.19.2 open3d-0.19.0 pyquaternion-0.9.9 retrying-1.3.4 werkzeug-3.0.6 widgetsnbextension-4.0.14\n"
          ]
        }
      ],
      "source": [
        "!pip install pillow-heif\n",
        "!pip install open3d\n",
        "\n",
        "import os\n",
        "import os.path as osp\n",
        "import numpy as np\n",
        "import open3d as o3d\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from typing import List,Dict,Tuple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "generate ground truth method (can skip if already mount gt folder)"
      ],
      "metadata": {
        "id": "qOnu9TaR7-j7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8RLpxYFgmcRP"
      },
      "outputs": [],
      "source": [
        "INTRINSINC = (525, 525, 320, 240)  # fx, fy, cx, cy\n",
        "\n",
        "def imread_cv2(path:str, options=cv2.IMREAD_COLOR):\n",
        "    \"\"\"Open an image or a depthmap with opencv-python.\"\"\"\n",
        "    if path.endswith((\".exr\", \"EXR\")):\n",
        "        options = cv2.IMREAD_ANYDEPTH\n",
        "    img = cv2.imread(path, options)\n",
        "    if img is None:\n",
        "        raise IOError(f\"Could not load image={path} with {options=}\")\n",
        "    if img.ndim == 3:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    return img\n",
        "\n",
        "def depthmap_to_world_coordinates(depthmap, camera_intrinsics, camera_pose, **kw):\n",
        "    \"\"\"\n",
        "    Projects a depth map into 3D world coordinates using camera intrinsics and optional pose.\n",
        "\n",
        "    Args:\n",
        "        depthmap (H x W): Depth values (in camera space).\n",
        "        intrinsics (3 x 3): Camera intrinsic matrix.\n",
        "        pose (optional, 4 x 4 or 4 x 3): Camera-to-world transformation.\n",
        "        pseudo_focal (optional, H x W): Per-pixel focal length override.\n",
        "\n",
        "    Returns:\n",
        "        pts_world (H x W x 3): 3D point cloud in world coordinates.\n",
        "        valid_mask (H x W): Boolean mask indicating valid (non-zero) depth pixels.\n",
        "    \"\"\"\n",
        "\n",
        "    H, W = depthmap.shape\n",
        "    camera_intrinsics = np.float32(camera_intrinsics)\n",
        "\n",
        "    # Extract intrinsic parameters\n",
        "    assert camera_intrinsics[0, 1] == 0.0 and camera_intrinsics[1, 0] == 0.0\n",
        "    fu,fv = camera_intrinsics[0, 0], camera_intrinsics[1, 1]\n",
        "    cu, cv = camera_intrinsics[0, 2], camera_intrinsics[1, 2]\n",
        "\n",
        "    # Generate pixel coordinate grid\n",
        "    u, v = np.meshgrid(np.arange(W), np.arange(H))  # u: cols, v: rows\n",
        "\n",
        "    # Backproject depth to 3D camera coordinates\n",
        "    z = depthmap\n",
        "    x = (u - cu) * z / fu\n",
        "    y = (v - cv) * z / fv\n",
        "    pts_cam = np.stack((x, y, z), axis=-1).astype(np.float32)\n",
        "\n",
        "    # Mark valid points (depth > 0)\n",
        "    valid_mask = z > 0.0\n",
        "\n",
        "    # Transform to world coordinates if pose is given\n",
        "    if camera_pose is not None:\n",
        "        R = camera_pose[:3, :3]\n",
        "        t = camera_pose[:3, 3]\n",
        "        pts_world = np.einsum(\"ik, vuk -> vui\", R, pts_cam) + t\n",
        "    else:\n",
        "        pts_world = pts_cam\n",
        "\n",
        "    return pts_world, valid_mask\n",
        "\n",
        "class SevenSceneSequence:\n",
        "    def __init__(\n",
        "            self,\n",
        "            seq_dir_path,\n",
        "        ):\n",
        "        self.seq_dir_path = seq_dir_path\n",
        "        # Find all the filenames end with \".color.png\"\n",
        "        # and check if corresponding \".proj.png\" and \".pose.txt\" exists\n",
        "\n",
        "        _color_files = [f for f in os.listdir(seq_dir_path) if f.endswith(\".color.png\")]\n",
        "        frame_names = [f.rstrip(\".color.png\") for f in _color_files]\n",
        "\n",
        "        self.valid_frame_names = []\n",
        "        for name in frame_names:\n",
        "            proj_path = osp.join(seq_dir_path, f\"{name}.depth.proj.png\")\n",
        "            pose_path = osp.join(seq_dir_path, f\"{name}.pose.txt\")\n",
        "            if osp.isfile(proj_path) and osp.isfile(pose_path):\n",
        "                self.valid_frame_names.append(name)\n",
        "        self.valid_frame_names = sorted(self.valid_frame_names)\n",
        "\n",
        "        print(f\"{len(self.valid_frame_names)} frames collected in {self.seq_dir_path}!!\")\n",
        "        print(f\"{len(_color_files) - len(self.valid_frame_names)} rgb frames miss .proj.png or .pose.txt!!\")\n",
        "\n",
        "\n",
        "    def get_views(self,kf_every = 200)->List[Dict]:\n",
        "\n",
        "        names = self.valid_frame_names[::kf_every] # select 1 out of every kf_every frames for reconstruction\n",
        "\n",
        "        views = []\n",
        "        \"\"\"\n",
        "        For each view(key frame), we compute the following metric\n",
        "        \"\"\"\n",
        "        for idx,name in enumerate(names):\n",
        "            view = dict()\n",
        "\n",
        "            impath = osp.join(self.seq_dir_path, f\"{name}.color.png\")\n",
        "            depthpath = osp.join(self.seq_dir_path, f\"{name}.depth.proj.png\")\n",
        "            posepath = osp.join(self.seq_dir_path, f\"{name}.pose.txt\")\n",
        "            view[\"name\"] = f'{self.seq_dir_path}/{name}'\n",
        "\n",
        "            rgb_image = imread_cv2(impath)\n",
        "            depthmap = imread_cv2(depthpath, cv2.IMREAD_UNCHANGED)\n",
        "            rgb_image = cv2.resize(rgb_image, (depthmap.shape[1], depthmap.shape[0]))\n",
        "\n",
        "            width, height = Image.fromarray(rgb_image).size\n",
        "            assert (width,height) == (640,480)\n",
        "            view['img'] = (rgb_image / 255.0 ).astype(np.float32)# Normalize to 0 to 1 for open3d format\n",
        "            view[\"true_shape\"] = np.int32((height, width))\n",
        "\n",
        "            depthmap[depthmap == 65535] = 0\n",
        "            depthmap = np.nan_to_num(depthmap.astype(np.float32), 0.0) / 1000.0\n",
        "            depthmap[depthmap > 10] = 0\n",
        "            depthmap[depthmap < 1e-3] = 0\n",
        "            assert np.isfinite(depthmap).all(), \\\n",
        "                f\"NaN in depthmap for view {view['name']}\"\n",
        "            view['depthmap'] = depthmap\n",
        "\n",
        "            camera_pose = np.loadtxt(posepath).astype(np.float32)\n",
        "            fx, fy, cx, cy = INTRINSINC ### NOTE: This intrinsic does not match with that on internet\n",
        "            intrinsics = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]], dtype=np.float32)\n",
        "            assert np.isfinite(camera_pose).all(), \\\n",
        "                f\"NaN in camera pose for view {view['name']}\"\n",
        "\n",
        "            view['camera_pose'] = camera_pose\n",
        "            view['camera_intrinsics'] = intrinsics\n",
        "\n",
        "            # encode the image\n",
        "            pts3d, valid_mask = depthmap_to_world_coordinates(**view)\n",
        "            view[\"pts3d\"] = pts3d\n",
        "            view[\"valid_mask\"] = valid_mask & np.isfinite(pts3d).all(axis=-1)\n",
        "            view[\"img_mask\"] = True\n",
        "\n",
        "            # check all datatypes\n",
        "            for key, val in view.items():\n",
        "                res, err_msg = self._is_good_type(key, val)\n",
        "                assert res, f\"{err_msg} with {key}={val} for view {view['name']}\"\n",
        "\n",
        "            views.append(view)\n",
        "\n",
        "        for view in views:\n",
        "            height, width = view['true_shape']\n",
        "            assert width >= height, ValueError(\"Width > Height\")\n",
        "\n",
        "        return views\n",
        "\n",
        "    def _is_good_type(self,key, v):\n",
        "        \"\"\"returns (is_good, err_msg)\"\"\"\n",
        "        if isinstance(v, (str, int, tuple)):\n",
        "            return True, None\n",
        "        if v.dtype not in (np.float32, bool, np.int32, np.int64, np.uint8):\n",
        "            return False, f\"bad {v.dtype=}\"\n",
        "        return True, None\n",
        "\n",
        "def seq2ply(seq_dir_path, ply_path, kf_every = 1, crop_size = None, voxel_grid_size = None):\n",
        "    \"\"\"\n",
        "    Converts a sequence of frames into a single 3D point cloud and saves it as a .ply file.\n",
        "\n",
        "    Parameters:\n",
        "        seq_dir_path (str): Path to the sequence directory. This directory should contain multiple\n",
        "                            frame subdirectories or files, each including:\n",
        "                                - .color.png: RGB image\n",
        "                                - .proj.png: Projected depth or coordinate image\n",
        "                                - .pose.txt: Camera pose matrix (usually 4x4)\n",
        "\n",
        "        ply_path (str): Destination path for the output .ply point cloud file.\n",
        "        kf_every (int): Selec key frame every \"kf_every\" frames for building points cloud\n",
        "\n",
        "    Description:\n",
        "        This function reads all frames in the given sequence directory, reconstructs 3D points using the color,\n",
        "        projection, and pose data, merges them into a single point cloud, and writes the result to\n",
        "        a .ply file.\n",
        "    \"\"\"\n",
        "    # Step 1: Collect the necessary information of frames for reconstruction\n",
        "    seq = SevenSceneSequence(seq_dir_path = seq_dir_path )\n",
        "    views = seq.get_views(kf_every = kf_every)\n",
        "    pts_gt_all, images_all,  masks_all = [], [], []\n",
        "\n",
        "    # Step 2: Only believe the central information of the camera\n",
        "    assert crop_size is None \\\n",
        "        or isinstance(crop_size, int), \\\n",
        "        \"crop_size must be None or an integer\"\n",
        "\n",
        "    for _, view in enumerate(views):\n",
        "        image = view[\"img\"]  # W,H,3\n",
        "        mask = view[\"valid_mask\"]    # W,H\n",
        "        pts_gt = view['pts3d'] # W,H,3\n",
        "\n",
        "        # Center on the given window size\n",
        "        if crop_size is not None:\n",
        "            H, W = image.shape[:2]\n",
        "            if crop_size > H or crop_size > W:\n",
        "                print(f\"Warning: Adjust crop_size({crop_size}) since it exceeds H({H}) or W({W})\")\n",
        "                crop_size = min(W,H)\n",
        "            _shift = crop_size//2\n",
        "            cx,cy = W // 2,H // 2\n",
        "            l, t = cx - _shift, cy - _shift # left, top\n",
        "            r, b = cx + _shift, cy + _shift # right, bottom\n",
        "\n",
        "            image = image[t:b, l:r]\n",
        "            mask = mask[t:b, l:r]\n",
        "            pts_gt = pts_gt[t:b, l:r]\n",
        "\n",
        "        #### Align predicted 3D points to the ground truth\n",
        "        images_all.append( image[None, ...] )\n",
        "        pts_gt_all.append( pts_gt[None, ...] )\n",
        "        masks_all.append( mask[None, ...] )\n",
        "\n",
        "\n",
        "    # Step 3: Build the 3D points map\n",
        "    images_all = np.concatenate(images_all, axis=0)\n",
        "    pts_gt_all = np.concatenate(pts_gt_all, axis=0)\n",
        "    masks_all = np.concatenate(masks_all, axis=0)\n",
        "    pts_gt_all_masked = pts_gt_all[masks_all > 0]\n",
        "    images_all_masked = images_all[masks_all > 0]\n",
        "\n",
        "    #save_params = {}\n",
        "    #save_params[\"images_all\"] = images_all\n",
        "    #save_params[\"pts_gt_all\"] = pts_gt_all\n",
        "    #save_params[\"masks_all\"] = masks_all\n",
        "    #np.save(_path_,save_params,)\n",
        "\n",
        "    pcd_gt = o3d.geometry.PointCloud()\n",
        "    pcd_gt.points = o3d.utility.Vector3dVector(\n",
        "        pts_gt_all_masked.reshape(-1, 3)\n",
        "    )\n",
        "    pcd_gt.colors = o3d.utility.Vector3dVector(\n",
        "        images_all_masked.reshape(-1, 3)\n",
        "    )\n",
        "    print(f'Points Cloud has {len(pcd_gt.points)} points')\n",
        "    if voxel_grid_size is not None:\n",
        "        pcd_gt = pcd_gt.voxel_down_sample(voxel_size=voxel_grid_size)\n",
        "        print(f'After downsample, Points Cloud has {len(pcd_gt.points)} points')\n",
        "\n",
        "    o3d.io.write_point_cloud(ply_path, pcd_gt, )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Xz4kbKpNqecX"
      },
      "outputs": [],
      "source": [
        "#if already mount gt folder than is fine to discard\n",
        "def generate_ground_truth_ply(scenes_root, pointcloud_root, scene_list, split='train',\n",
        "                               kf_every=20, voxel_grid_size=0.0075, enable=True):\n",
        "\n",
        "    if not enable:\n",
        "        print(f\"[INFO] Ground truth generation for split '{split}' is disabled.\")\n",
        "        return\n",
        "    os.makedirs(pointcloud_root, exist_ok=True)\n",
        "\n",
        "    for scene in scene_list:\n",
        "        root_path = osp.join(scenes_root, scene)\n",
        "        split_path = osp.join(root_path, split)\n",
        "        split_txt = osp.join(root_path, f\"{split.capitalize()}Split.txt\")\n",
        "\n",
        "        if not osp.isfile(split_txt):\n",
        "            print(f\"[WARNING] {split_txt} 不存在，跳過 scene {scene}\")\n",
        "            continue\n",
        "\n",
        "        with open(split_txt, \"r\") as f:\n",
        "            seq_names = [line.strip() for line in f.readlines()]\n",
        "\n",
        "        for seq in seq_names:\n",
        "            seq_num = int(seq.replace(\"sequence\", \"\"))\n",
        "            seq_dir = osp.join(split_path, f\"seq-{seq_num:02d}\")\n",
        "            if not osp.isdir(seq_dir):\n",
        "                print(f\"[WARNING] 資料夾不存在：{seq_dir}，跳過\")\n",
        "                continue\n",
        "\n",
        "            ply_path = osp.join(pointcloud_root, f\"{scene}-seq-{seq_num}.ply\")\n",
        "            if osp.isfile(ply_path):\n",
        "                print(f\"[SKIP] 已存在：{ply_path}\")\n",
        "                continue\n",
        "\n",
        "            print(f\"[INFO] 正在處理：{scene} - seq-{seq_num:02d}\")\n",
        "            seq2ply(seq_dir, ply_path, kf_every=kf_every, voxel_grid_size=voxel_grid_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fast3r load image method"
      ],
      "metadata": {
        "id": "P31ej44L71oh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9qZTCXPCoMMg"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as tvf\n",
        "import PIL.Image\n",
        "from pillow_heif import register_heif_opener\n",
        "import re\n",
        "from PIL import ExifTags\n",
        "def exif_transpose(image: Image.Image, *, in_place: bool = False) -> Image.Image | None:\n",
        "    \"\"\"\n",
        "    If an image has an EXIF Orientation tag, other than 1, transpose the image\n",
        "    accordingly, and remove the orientation data.\n",
        "\n",
        "    :param image: The image to transpose.\n",
        "    :param in_place: Boolean. Keyword-only argument.\n",
        "        If ``True``, the original image is modified in-place, and ``None`` is returned.\n",
        "        If ``False`` (default), a new :py:class:`~PIL.Image.Image` object is returned\n",
        "        with the transposition applied. If there is no transposition, a copy of the\n",
        "        image will be returned.\n",
        "    \"\"\"\n",
        "    image.load()\n",
        "    image_exif = image.getexif()\n",
        "    orientation = image_exif.get(ExifTags.Base.Orientation, 1)\n",
        "    method = {\n",
        "        2: Image.Transpose.FLIP_LEFT_RIGHT,\n",
        "        3: Image.Transpose.ROTATE_180,\n",
        "        4: Image.Transpose.FLIP_TOP_BOTTOM,\n",
        "        5: Image.Transpose.TRANSPOSE,\n",
        "        6: Image.Transpose.ROTATE_270,\n",
        "        7: Image.Transpose.TRANSVERSE,\n",
        "        8: Image.Transpose.ROTATE_90,\n",
        "    }.get(orientation)\n",
        "    if method is not None:\n",
        "        if in_place:\n",
        "            image.im = image.im.transpose(method)\n",
        "            image._size = image.im.size\n",
        "        else:\n",
        "            transposed_image = image.transpose(method)\n",
        "        exif_image = image if in_place else transposed_image\n",
        "\n",
        "        exif = exif_image.getexif()\n",
        "        if ExifTags.Base.Orientation in exif:\n",
        "            del exif[ExifTags.Base.Orientation]\n",
        "            if \"exif\" in exif_image.info:\n",
        "                exif_image.info[\"exif\"] = exif.tobytes()\n",
        "            elif \"Raw profile type exif\" in exif_image.info:\n",
        "                exif_image.info[\"Raw profile type exif\"] = exif.tobytes().hex()\n",
        "            for key in (\"XML:com.adobe.xmp\", \"xmp\"):\n",
        "                if key in exif_image.info:\n",
        "                    for pattern in (\n",
        "                        r'tiff:Orientation=\"([0-9])\"',\n",
        "                        r\"<tiff:Orientation>([0-9])</tiff:Orientation>\",\n",
        "                    ):\n",
        "                        value = exif_image.info[key]\n",
        "                        exif_image.info[key] = (\n",
        "                            re.sub(pattern, \"\", value)\n",
        "                            if isinstance(value, str)\n",
        "                            else re.sub(pattern.encode(), b\"\", value)\n",
        "                        )\n",
        "        if not in_place:\n",
        "            return transposed_image\n",
        "    elif not in_place:\n",
        "        return image.copy()\n",
        "    return None\n",
        "def _resize_pil_image(img, long_edge_size):\n",
        "    S = max(img.size)\n",
        "    if S > long_edge_size:\n",
        "        interp = PIL.Image.LANCZOS\n",
        "    elif S <= long_edge_size:\n",
        "        interp = PIL.Image.BICUBIC\n",
        "    new_size = tuple(int(round(x * long_edge_size / S)) for x in img.size)\n",
        "    return img.resize(new_size, interp)\n",
        "\n",
        "def load_images(folder_or_list, size, square_ok=False, verbose=True, rotate_clockwise_90=False, crop_to_landscape=False):\n",
        "    \"\"\"open and convert all images in a list or folder to proper input format for DUSt3R\"\"\"\n",
        "    ImgNorm = tvf.Compose([tvf.ToTensor(), tvf.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "    try:\n",
        "        register_heif_opener()\n",
        "        heif_support_enabled = True\n",
        "    except ImportError:\n",
        "        heif_support_enabled = False\n",
        "\n",
        "    if isinstance(folder_or_list, str):\n",
        "        if verbose:\n",
        "            print(f\">> Loading images from {folder_or_list}\")\n",
        "        root, folder_content = folder_or_list, sorted(os.listdir(folder_or_list))\n",
        "\n",
        "    elif isinstance(folder_or_list, list):\n",
        "        if verbose:\n",
        "            print(f\">> Loading a list of {len(folder_or_list)} images\")\n",
        "        root, folder_content = \"\", folder_or_list\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"bad {folder_or_list=} ({type(folder_or_list)})\")\n",
        "\n",
        "    supported_images_extensions = [\".jpg\", \".jpeg\", \".png\"]\n",
        "    if heif_support_enabled:\n",
        "        supported_images_extensions += [\".heic\", \".heif\"]\n",
        "    supported_images_extensions = tuple(supported_images_extensions)\n",
        "\n",
        "    imgs = []\n",
        "    for path in folder_content:\n",
        "        if not path.lower().endswith(supported_images_extensions):\n",
        "            continue\n",
        "        img = exif_transpose(PIL.Image.open(os.path.join(root, path))).convert(\"RGB\")\n",
        "        if rotate_clockwise_90:\n",
        "            img = img.rotate(-90, expand=True)\n",
        "        if crop_to_landscape:\n",
        "            # Crop to a landscape aspect ratio (e.g., 16:9)\n",
        "            desired_aspect_ratio = 4 / 3\n",
        "            width, height = img.size\n",
        "            current_aspect_ratio = width / height\n",
        "\n",
        "            if current_aspect_ratio > desired_aspect_ratio:\n",
        "                # Wider than landscape: crop width\n",
        "                new_width = int(height * desired_aspect_ratio)\n",
        "                left = (width - new_width) // 2\n",
        "                right = left + new_width\n",
        "                top = 0\n",
        "                bottom = height\n",
        "            else:\n",
        "                # Taller than landscape: crop height\n",
        "                new_height = int(width / desired_aspect_ratio)\n",
        "                top = (height - new_height) // 2\n",
        "                bottom = top + new_height\n",
        "                left = 0\n",
        "                right = width\n",
        "\n",
        "            img = img.crop((left, top, right, bottom))\n",
        "\n",
        "        W1, H1 = img.size\n",
        "        if size == 224:\n",
        "            # resize short side to 224 (then crop)\n",
        "            img = _resize_pil_image(img, round(size * max(W1 / H1, H1 / W1)))\n",
        "        else:\n",
        "            # resize long side to 512\n",
        "            img = _resize_pil_image(img, size)\n",
        "        W, H = img.size\n",
        "        cx, cy = W // 2, H // 2\n",
        "        if size == 224:\n",
        "            half = min(cx, cy)\n",
        "            img = img.crop((cx - half, cy - half, cx + half, cy + half))\n",
        "        else:\n",
        "            halfw, halfh = ((2 * cx) // 16) * 8, ((2 * cy) // 16) * 8\n",
        "            if not (square_ok) and W == H:\n",
        "                halfh = 3 * halfw / 4\n",
        "            img = img.crop((cx - halfw, cy - halfh, cx + halfw, cy + halfh))\n",
        "\n",
        "        W2, H2 = img.size\n",
        "        if verbose:\n",
        "            print(f\" - adding {path} with resolution {W1}x{H1} --> {W2}x{H2}\")\n",
        "        imgs.append(\n",
        "            dict(\n",
        "                img=ImgNorm(img)[None],\n",
        "                true_shape=np.int32([img.size[::-1]]),\n",
        "                idx=len(imgs),\n",
        "                instance=str(len(imgs)),\n",
        "            )\n",
        "        )\n",
        "\n",
        "    assert imgs, \"no images foud at \" + root\n",
        "    if verbose:\n",
        "        print(f\" (Found {len(imgs)} images)\")\n",
        "    return imgs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "define dataset"
      ],
      "metadata": {
        "id": "37ySgRgM7uyA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "Tq3TnnkvvFZg"
      },
      "outputs": [],
      "source": [
        "#generate train/test dataset\n",
        "def get_keyframe_paths(seq_dir, kf_every=20):\n",
        "    image_paths = []\n",
        "    for fname in sorted(os.listdir(seq_dir)):\n",
        "        if fname.endswith(\".color.png\"):\n",
        "            frame_id = int(fname.replace(\"frame-\", \"\").replace(\".color.png\", \"\"))\n",
        "            if frame_id % kf_every == 0:\n",
        "                image_paths.append(os.path.join(seq_dir, fname)\n",
        "                )\n",
        "\n",
        "    return image_paths\n",
        "def load_depth_cv2(path, size, square_ok=False):\n",
        "    depth = cv2.imread(path, cv2.IMREAD_UNCHANGED).astype(np.float32)\n",
        "    depth[depth == 65535] = np.nan\n",
        "    depth /= 1000.0\n",
        "    depth[(depth < 1e-3) | (depth > 10.0)] = np.nan\n",
        "\n",
        "    H1, W1 = depth.shape\n",
        "\n",
        "    # Resize：與 load_images 對齊\n",
        "    if size == 224:\n",
        "        # resize short side to 224\n",
        "        scale = round(size * max(W1 / H1, H1 / W1)) / max(W1, H1)\n",
        "        new_size = (int(W1 * scale), int(H1 * scale))\n",
        "    else:\n",
        "        # resize long side to size\n",
        "        scale = size / max(W1, H1)\n",
        "        new_size = (int(W1 * scale), int(H1 * scale))\n",
        "\n",
        "    depth = cv2.resize(depth, new_size, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    # Center crop\n",
        "    H, W = depth.shape\n",
        "    cx, cy = W // 2, H // 2\n",
        "    if size == 224:\n",
        "        half = min(cx, cy)\n",
        "        depth = depth[cy - half:cy + half, cx - half:cx + half]\n",
        "    else:\n",
        "        halfw = ((2 * cx) // 16) * 8\n",
        "        halfh = ((2 * cy) // 16) * 8\n",
        "        if not square_ok and W == H:\n",
        "            halfh = int(3 * halfw / 4)\n",
        "        depth = depth[cy - halfh:cy + halfh, cx - halfw:cx + halfw]\n",
        "\n",
        "\n",
        "    mask = (~np.isnan(depth)).astype(np.float32)\n",
        "    depth = np.nan_to_num(depth, nan=0.0)\n",
        "\n",
        "    return depth, mask\n",
        "\n",
        "class MultiViewPointCloudDataset(Dataset):\n",
        "    def __init__(self, scenes_root, pointcloud_root, scene_list, kf_every=20, views_per_sample=5, size=384, split='train'):\n",
        "\n",
        "        assert split in ['train', 'test'], \"split must be 'train' or 'test'\"\n",
        "        self.samples = []\n",
        "        self.size = size\n",
        "        self.views_per_sample = views_per_sample\n",
        "\n",
        "        for scene in scene_list:\n",
        "            scene_path = osp.join(scenes_root, scene)\n",
        "            split_dir = osp.join(scene_path, split)\n",
        "            split_txt = osp.join(scene_path, f'{split.capitalize()}Split.txt')\n",
        "\n",
        "            with open(split_txt, \"r\") as f:\n",
        "                seq_names = [line.strip() for line in f.readlines()]\n",
        "\n",
        "            for seq in seq_names:\n",
        "                seq_num = int(seq.replace(\"sequence\", \"\"))\n",
        "                seq_dir = osp.join(split_dir, f\"seq-{seq_num:02d}\")\n",
        "                ply_path = osp.join(pointcloud_root, f\"{scene}-seq-{seq_num:02d}.ply\")\n",
        "\n",
        "                if not osp.isdir(seq_dir) :\n",
        "                    continue\n",
        "\n",
        "                # image_paths = get_keyframe_paths(seq_dir, kf_every=kf_every)\n",
        "                # if len(image_paths) < views_per_sample:\n",
        "                #     continue\n",
        "\n",
        "                # self.samples.append({\n",
        "                #     \"image_paths\": image_paths[:views_per_sample],\n",
        "                #     \"ply_path\": ply_path\n",
        "                # })\n",
        "                image_paths = get_keyframe_paths(seq_dir, kf_every=kf_every)\n",
        "\n",
        "                if len(image_paths) > views_per_sample:\n",
        "                    image_paths = image_paths[:views_per_sample]\n",
        "\n",
        "                self.samples.append({\n",
        "                    \"image_paths\": image_paths,\n",
        "                    \"ply_path\": ply_path\n",
        "                })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        image_paths = sample[\"image_paths\"]\n",
        "        ply_path = sample[\"ply_path\"]\n",
        "        depth_paths = [path.replace(\"color.png\", \"depth.proj.png\") for path in image_paths]\n",
        "        depthmaps = []\n",
        "        valid_masks = []\n",
        "        for path in depth_paths:\n",
        "\n",
        "            depth, mask = load_depth_cv2(path, self.size)\n",
        "            depthmaps.append(torch.from_numpy(depth[None]))     # [1, H, W]\n",
        "            valid_masks.append(torch.from_numpy(mask[None]))     # [1, H, W]\n",
        "\n",
        "        depths = torch.stack(depthmaps)       # [V, 1, H, W]\n",
        "        masks = torch.stack(valid_masks)      # [V, 1, H, W]\n",
        "\n",
        "        images = load_images(image_paths, size=self.size, verbose=False)\n",
        "        images = torch.stack([img_dict[\"img\"].squeeze(0) for img_dict in images])\n",
        "        gt_pcd = o3d.io.read_point_cloud(ply_path)\n",
        "        gt_points = np.asarray(gt_pcd.points).astype(np.float32)  # (N, 3)\n",
        "\n",
        "        return {\n",
        "            \"images\": images,              # [V, 3, H, W] or list\n",
        "            \"depths\": depths,              # [V, 1, H, W]\n",
        "            \"masks\": masks,                # [V, 1, H, W]\n",
        "            \"target_pointcloud\": torch.from_numpy(gt_points),  # [N, 3]\n",
        "            \"image_paths\": image_paths\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "0A1zE-NU7Re7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lei3Q-Bvv2oJ"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import resnet18\n",
        "from torchvision.models.feature_extraction import create_feature_extractor\n",
        "#model resnet+ace\n",
        "class ACEHead(nn.Module):\n",
        "    def __init__(self, in_channels=512, mid_channels=256):\n",
        "        super().__init__()\n",
        "        # dense block + skip block (parallel)\n",
        "        self.skip = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, 1),\n",
        "            nn.PReLU(),\n",
        "            nn.Conv2d(mid_channels, mid_channels, 1),\n",
        "            nn.PReLU(),\n",
        "        )\n",
        "        self.dense = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, 1),\n",
        "            nn.PReLU(),\n",
        "            nn.Conv2d(mid_channels, mid_channels, 1),\n",
        "            nn.PReLU(),\n",
        "        )\n",
        "\n",
        "        self.eca = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),                  # [B, C, 1, 1]\n",
        "            nn.Conv2d(mid_channels * 2, 1, kernel_size=1),  # 用 Conv2d 而不是 Conv1d\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.output_layer = nn.Conv2d(mid_channels * 2, 4, 1)  # [x, y, z, w_hat]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.skip(x)\n",
        "        x2 = self.dense(x)\n",
        "        x_cat = torch.cat([x1, x2], dim=1)\n",
        "\n",
        "        # channel attention\n",
        "        attn = self.eca(x_cat).view(x_cat.shape[0], -1, 1, 1)\n",
        "        x_attn = x_cat * attn\n",
        "\n",
        "        return self.output_layer(x_attn)\n",
        "\n",
        "\n",
        "class ResNetBackbone(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_layer='layer4'):\n",
        "        super().__init__()\n",
        "        model = resnet18(pretrained=True)\n",
        "        model.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "        model.layer4[0].conv1.stride = (1, 1)\n",
        "        model.layer4[0].downsample[0].stride = (1, 1)\n",
        "\n",
        "        return_nodes = {out_layer: \"features\"}\n",
        "        self.backbone = create_feature_extractor(model, return_nodes=return_nodes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)[\"features\"]  # [B, 512, H', W']\n",
        "\n",
        "class RGBDPointPredictor(nn.Module):\n",
        "    def __init__(self, output_size=(288, 384)):\n",
        "        super().__init__()\n",
        "        self.rgb_backbone = ResNetBackbone(in_channels=3)\n",
        "        self.depth_backbone = ResNetBackbone(in_channels=1)\n",
        "        self.fusion_conv = nn.Conv2d(512 * 2, 512, kernel_size=1)  # fuse channel-wise\n",
        "        self.head = ACEHead(in_channels=512)\n",
        "        self.output_size = output_size  # (H, W)\n",
        "\n",
        "    def forward(self, rgb, depth):\n",
        "        \"\"\"\n",
        "        rgb: [B, 3, H, W]  (e.g., [B, 3, 288, 384])\n",
        "        depth: [B, 1, H, W]\n",
        "        \"\"\"\n",
        "        rgb_feat = self.rgb_backbone(rgb)      # [B, 512, H', W']\n",
        "        dpt_feat = self.depth_backbone(depth)  # [B, 512, H', W']\n",
        "        feat = torch.cat([rgb_feat, dpt_feat], dim=1)  # [B, 1024, H', W']\n",
        "        fused = self.fusion_conv(feat)                # [B, 512, H', W']\n",
        "        output = self.head(fused)                     # [B, 4, H', W']\n",
        "\n",
        "        output_upsampled = F.interpolate(\n",
        "            output,\n",
        "            size=self.output_size,\n",
        "            mode=\"bilinear\",\n",
        "            align_corners=False\n",
        "        )  # [B, 4, 288, 384]\n",
        "\n",
        "        return output_upsampled\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "loss function, accuracy and completeness"
      ],
      "metadata": {
        "id": "zNMktq1S7DqG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nfANLEiHv996"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial import cKDTree\n",
        "def chamfer_distance(p1, p2):\n",
        "    \"\"\"\n",
        "    Chamfer Distance between two point clouds without batch dim.\n",
        "\n",
        "    Args:\n",
        "        p1: Tensor (P1, D)\n",
        "        p2: Tensor (P2, D)\n",
        "\n",
        "    Returns:\n",
        "        scalar loss\n",
        "    \"\"\"\n",
        "    diff = p1.unsqueeze(1) - p2.unsqueeze(0)   # (P1, P2, D)\n",
        "    dist = torch.sum(diff ** 2, dim=-1)        # (P1, P2)\n",
        "\n",
        "    min_dist_p1, _ = torch.min(dist, dim=1)    # (P1,)\n",
        "    min_dist_p2, _ = torch.min(dist, dim=0)    # (P2,)\n",
        "\n",
        "    loss = min_dist_p1.mean() + min_dist_p2.mean()\n",
        "    return loss\n",
        "def point_cloud_accuracy(pred_points: np.ndarray, gt_points: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Median distance from each predicted point to its nearest ground truth point.\n",
        "    \"\"\"\n",
        "    tree = cKDTree(gt_points)\n",
        "    distances, _ = tree.query(pred_points, k=1)\n",
        "    return np.median(distances)\n",
        "\n",
        "def point_cloud_completeness(pred_points: np.ndarray, gt_points: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Median distance from each ground-truth point to its nearest predicted point.\n",
        "    \"\"\"\n",
        "    tree = cKDTree(pred_points)\n",
        "    distances, _ = tree.query(gt_points, k=1)\n",
        "    return np.median(distances)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "define point cloud sampling method and evaluate/train function"
      ],
      "metadata": {
        "id": "YYx6y3ft63Y7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lUq50MLFrEYS"
      },
      "outputs": [],
      "source": [
        "# train/test dependency\n",
        "def random_sampling(points, num_samples=2048):\n",
        "\n",
        "    if points.shape[0] > num_samples:\n",
        "        idx = torch.randperm(points.shape[0])[:num_samples]\n",
        "        return points[idx]\n",
        "    else:\n",
        "        return points\n",
        "\n",
        "def random_sampling_ratio(points, ratio=1.0):\n",
        "    \"\"\"\n",
        "    Randomly samples a portion of points based on a ratio.\n",
        "\n",
        "    Args:\n",
        "        points (torch.Tensor): Tensor of shape (N, D) representing N points.\n",
        "        ratio (float): Sampling ratio in (0, 1]. If >= 1.0, return all points.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Sampled points.\n",
        "    \"\"\"\n",
        "    num_points = points.shape[0]\n",
        "\n",
        "    if ratio >= 1.0:\n",
        "        return points\n",
        "    elif ratio <= 0.0:\n",
        "        raise ValueError(\"Sampling ratio must be > 0.\")\n",
        "\n",
        "    num_samples = int(num_points * ratio)\n",
        "    idx = torch.randperm(num_points)[:num_samples]\n",
        "    return points[idx]\n",
        "\n",
        "def extract_scene_and_seq(path):\n",
        "    # path: './7SCENES/stairs/train/seq-06/frame-000000.color.png'\n",
        "    parts = path.split(os.sep)\n",
        "    scene = parts[-4]               # 'stairs'\n",
        "    sequence_id = int(parts[-2].split('-')[1])  # '06' → 6\n",
        "    return scene, sequence_id\n",
        "def evaluate(model, loader, device, desc=\"Evaluation\", save_results=False, save_dir=\"./test\"):\n",
        "  model.eval()\n",
        "  total_loss = 0\n",
        "  total_acc = 0\n",
        "  total_comp = 0\n",
        "  with torch.no_grad():\n",
        "      for data in tqdm(loader, desc=desc, leave=False):\n",
        "          W = data[\"images\"].shape[-1]\n",
        "          H = data[\"images\"].shape[-2]\n",
        "          images = data[\"images\"].view(-1, 3, H, W).to(device)\n",
        "          depths = data[\"depths\"].view(-1, 1, H, W).to(device)\n",
        "          target_pcd = data[\"target_pointcloud\"].to(device)\n",
        "\n",
        "          pred = model(images, depths)\n",
        "          xyz = pred[:, :3].permute(0, 2, 3, 1).reshape(-1, 3)\n",
        "\n",
        "          # first_pose\n",
        "          # first_pose = load_first_pose(data[\"image_paths\"][0][0])  # (4,4) numpy\n",
        "          # R0 = torch.from_numpy(first_pose[:3, :3]).to(xyz)        # (3,3)\n",
        "          # t0 = torch.from_numpy(first_pose[:3, 3]).to(xyz)         # (3,)\n",
        "          # xyz_world = (xyz @ R0.T) + t0        # (N,3) → 世界座標\n",
        "\n",
        "          xyz_sampled = random_sampling(xyz, 8192) # xyz -> xyz_world\n",
        "          target_sampled = random_sampling(target_pcd[0], 20000)\n",
        "          #target_sampled = random_sampling_ratio(target_pcd[0],0.005)\n",
        "\n",
        "          loss = chamfer_distance(xyz_sampled, target_sampled)\n",
        "          acc = point_cloud_accuracy(xyz_sampled.detach().cpu().numpy(), target_sampled.detach().cpu().numpy())\n",
        "          comp = point_cloud_completeness(xyz_sampled.detach().cpu().numpy(), target_sampled.detach().cpu().numpy())\n",
        "\n",
        "          total_loss += loss.item()\n",
        "          total_acc += acc\n",
        "          total_comp += comp\n",
        "          # Save prediction as .ply file\n",
        "          if save_results:\n",
        "              scene, sequence_id = extract_scene_and_seq(data[\"image_paths\"][0][0])\n",
        "              save_path = os.path.join(save_dir, f\"{scene}-seq-{sequence_id:02d}.ply\")\n",
        "              print(f\"{scene}{sequence_id:02d} acc: {acc:.4f}, comp: {comp:.4f}\")\n",
        "\n",
        "              pcd = o3d.geometry.PointCloud()\n",
        "              pcd.points = o3d.utility.Vector3dVector(xyz_sampled.detach().cpu().numpy())\n",
        "              o3d.io.write_point_cloud(save_path, pcd)\n",
        "\n",
        "  num_batches = len(loader)\n",
        "  return total_loss / num_batches, total_acc / num_batches, total_comp / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oiD97FHGwFTf"
      },
      "outputs": [],
      "source": [
        "# train.py\n",
        "from torch.utils.data import random_split\n",
        "from tqdm import tqdm\n",
        "import argparse\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "def train(device):\n",
        "    dataset = MultiViewPointCloudDataset(\n",
        "        scenes_root=SCENES_ROOT,\n",
        "        pointcloud_root=pointcloud_root_train,\n",
        "        scene_list = ['chess', 'fire', 'heads', 'office', 'pumpkin', 'redkitchen', 'stairs'],\n",
        "        kf_every=10,\n",
        "        views_per_sample=10, #total choose img number\n",
        "        size=384,\n",
        "        split='train'\n",
        "    )\n",
        "\n",
        "    val_size = int(0.3 * len(dataset))\n",
        "    train_size = len(dataset) - val_size\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
        "\n",
        "    model = RGBDPointPredictor().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
        "    save_dir = \"./checkpoints\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    num_epochs = 100\n",
        "    patience = 10  # early stop patience\n",
        "    best_val_loss = float(\"inf\")\n",
        "    best_epoch = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss, total_acc, total_comp = 0, 0, 0\n",
        "        progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        for data in progress:\n",
        "            W, H = data[\"images\"].shape[-1], data[\"images\"].shape[-2]\n",
        "            images = data[\"images\"].view(-1, 3, H, W).to(device)\n",
        "            depths = data[\"depths\"].view(-1, 1, H, W).to(device)\n",
        "            target_pcd = data[\"target_pointcloud\"].to(device)\n",
        "\n",
        "            pred = model(images, depths)\n",
        "            xyz = pred[:, :3].permute(0, 2, 3, 1).reshape(-1, 3)\n",
        "            xyz_sampled = random_sampling(xyz, 2048)\n",
        "            target_sampled = random_sampling(target_pcd[0], 8192)\n",
        "\n",
        "            loss = chamfer_distance(xyz_sampled, target_sampled)\n",
        "            acc = point_cloud_accuracy(xyz_sampled.detach().cpu().numpy(), target_sampled.detach().cpu().numpy())\n",
        "            comp = point_cloud_completeness(xyz_sampled.detach().cpu().numpy(), target_sampled.detach().cpu().numpy())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_acc += acc\n",
        "            total_comp += comp\n",
        "            progress.set_postfix(loss=loss.item(), acc=acc, comp=comp)\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        avg_acc = total_acc / len(train_loader)\n",
        "        avg_comp = total_comp / len(train_loader)\n",
        "        val_loss, val_acc, val_comp = evaluate(model, val_loader, device, desc=\"Validation\")\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "\n",
        "        print(f\"[Epoch {epoch+1}] Train Loss: {avg_loss:.4f} | Acc: {avg_acc:.4f} | Comp: {avg_comp:.4f}\")\n",
        "        print(f\"               Val   Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | Comp: {val_comp:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_epoch = epoch + 1\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), f\"{save_dir}/best_model.pth\")\n",
        "            print(f\"Saved new best model at epoch {epoch+1}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"No improvement. Patience: {patience_counter}/{patience}\")\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}. Best model from epoch {best_epoch}\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "2m79mTrNruIR"
      },
      "outputs": [],
      "source": [
        "#test and create folder\n",
        "def test(device):\n",
        "    test_dataset = MultiViewPointCloudDataset(\n",
        "        scenes_root=SCENES_ROOT,\n",
        "        pointcloud_root=pointcloud_root_test,\n",
        "        scene_list = ['chess', 'fire', 'heads', 'office', 'pumpkin', 'redkitchen', 'stairs'],\n",
        "        kf_every=20,\n",
        "        views_per_sample=30,\n",
        "        size=384,\n",
        "        split='test'\n",
        "    )\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
        "\n",
        "    model = RGBDPointPredictor().to(device)\n",
        "    model.load_state_dict(torch.load(\"./checkpoints/best_model.pth\"))\n",
        "    test_loss, test_acc, test_comp = evaluate(\n",
        "        model, test_loader, device,\n",
        "        desc=\"Test\",\n",
        "        save_results=True,\n",
        "        save_dir=TEST_DIR\n",
        "    )\n",
        "\n",
        "    print(f\"[Test] Loss: {test_loss:.4f} | Acc: {test_acc:.4f} | Comp: {test_comp:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "generate ground truth point cloud (skip if already mount gt folder)"
      ],
      "metadata": {
        "id": "QZaYKEdE7Yy6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prV4muxSqrWb"
      },
      "outputs": [],
      "source": [
        "scene_list = ['chess', 'fire', 'heads', 'office', 'pumpkin', 'redkitchen', 'stairs']\n",
        "generate_ground_truth_ply(\n",
        "    scenes_root=SCENES_ROOT,\n",
        "    pointcloud_root=pointcloud_root_train,\n",
        "    scene_list=scene_list,\n",
        "    split=\"train\",\n",
        "    enable=True\n",
        ")\n",
        "\n",
        "# test split GT\n",
        "generate_ground_truth_ply(\n",
        "    scenes_root=SCENES_ROOT,\n",
        "    pointcloud_root=pointcloud_root_test,\n",
        "    scene_list=scene_list,\n",
        "    split=\"test\",\n",
        "    enable=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "training"
      ],
      "metadata": {
        "id": "7JwUkj-_6tkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "train(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poT56gMDVTy6",
        "outputId": "1e58f49a-3775-446a-9077-996fd0e82813"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 231MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Epoch 1/100: 100%|██████████| 20/20 [01:31<00:00,  4.58s/it, acc=0.321, comp=0.337, loss=0.588]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] Train Loss: 1.4121 | Acc: 0.4381 | Comp: 0.6453\n",
            "               Val   Loss: 2.1085 | Acc: 0.4762 | Comp: 0.3908\n",
            "Saved new best model at epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/100: 100%|██████████| 20/20 [00:06<00:00,  3.12it/s, acc=0.169, comp=0.186, loss=0.238]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 2] Train Loss: 0.4529 | Acc: 0.2503 | Comp: 0.2889\n",
            "               Val   Loss: 0.6779 | Acc: 0.2910 | Comp: 0.4319\n",
            "Saved new best model at epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/100: 100%|██████████| 20/20 [00:06<00:00,  3.10it/s, acc=0.162, comp=0.288, loss=0.555]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 3] Train Loss: 0.4877 | Acc: 0.2210 | Comp: 0.2952\n",
            "               Val   Loss: 0.4375 | Acc: 0.2634 | Comp: 0.2657\n",
            "Saved new best model at epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/100: 100%|██████████| 20/20 [00:06<00:00,  2.88it/s, acc=0.164, comp=0.145, loss=0.205]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 4] Train Loss: 0.3048 | Acc: 0.1785 | Comp: 0.2340\n",
            "               Val   Loss: 0.4215 | Acc: 0.2554 | Comp: 0.2516\n",
            "Saved new best model at epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/100: 100%|██████████| 20/20 [00:06<00:00,  2.93it/s, acc=0.262, comp=0.225, loss=0.314]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 5] Train Loss: 0.2369 | Acc: 0.1764 | Comp: 0.2261\n",
            "               Val   Loss: 0.5469 | Acc: 0.3239 | Comp: 0.2048\n",
            "No improvement. Patience: 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/100: 100%|██████████| 20/20 [00:06<00:00,  3.16it/s, acc=0.13, comp=0.218, loss=0.161]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 6] Train Loss: 0.2611 | Acc: 0.1898 | Comp: 0.2132\n",
            "               Val   Loss: 7.3024 | Acc: 0.3479 | Comp: 0.2226\n",
            "No improvement. Patience: 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/100: 100%|██████████| 20/20 [00:06<00:00,  3.09it/s, acc=0.171, comp=0.197, loss=0.224]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 7] Train Loss: 0.2740 | Acc: 0.1946 | Comp: 0.2206\n",
            "               Val   Loss: 0.4205 | Acc: 0.3304 | Comp: 0.1772\n",
            "Saved new best model at epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/100: 100%|██████████| 20/20 [00:06<00:00,  3.04it/s, acc=0.14, comp=0.128, loss=0.0962]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 8] Train Loss: 0.2190 | Acc: 0.1744 | Comp: 0.2053\n",
            "               Val   Loss: 0.4642 | Acc: 0.3396 | Comp: 0.1585\n",
            "No improvement. Patience: 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/100: 100%|██████████| 20/20 [00:06<00:00,  3.06it/s, acc=0.299, comp=0.357, loss=0.448]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 9] Train Loss: 0.2407 | Acc: 0.1865 | Comp: 0.2077\n",
            "               Val   Loss: 0.3474 | Acc: 0.2249 | Comp: 0.2712\n",
            "Saved new best model at epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/100: 100%|██████████| 20/20 [00:06<00:00,  3.00it/s, acc=0.161, comp=0.195, loss=0.33]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 10] Train Loss: 0.2051 | Acc: 0.1684 | Comp: 0.1989\n",
            "               Val   Loss: 0.6809 | Acc: 0.3324 | Comp: 0.2639\n",
            "No improvement. Patience: 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/100: 100%|██████████| 20/20 [00:06<00:00,  3.22it/s, acc=0.143, comp=0.224, loss=0.213]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 11] Train Loss: 0.2736 | Acc: 0.1930 | Comp: 0.2262\n",
            "               Val   Loss: 0.5801 | Acc: 0.3010 | Comp: 0.2499\n",
            "No improvement. Patience: 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/100: 100%|██████████| 20/20 [00:06<00:00,  3.08it/s, acc=0.17, comp=0.462, loss=0.429]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 12] Train Loss: 0.2818 | Acc: 0.1864 | Comp: 0.2518\n",
            "               Val   Loss: 0.4259 | Acc: 0.2202 | Comp: 0.3072\n",
            "No improvement. Patience: 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/100: 100%|██████████| 20/20 [00:06<00:00,  3.22it/s, acc=0.229, comp=0.278, loss=0.253]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 13] Train Loss: 0.3106 | Acc: 0.1693 | Comp: 0.2332\n",
            "               Val   Loss: 0.3503 | Acc: 0.2118 | Comp: 0.2075\n",
            "No improvement. Patience: 4/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/100: 100%|██████████| 20/20 [00:06<00:00,  3.08it/s, acc=0.148, comp=0.201, loss=0.155]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 14] Train Loss: 0.1825 | Acc: 0.1601 | Comp: 0.1940\n",
            "               Val   Loss: 0.2840 | Acc: 0.1875 | Comp: 0.1854\n",
            "Saved new best model at epoch 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/100: 100%|██████████| 20/20 [00:06<00:00,  2.95it/s, acc=0.165, comp=0.197, loss=0.148]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 15] Train Loss: 0.1588 | Acc: 0.1490 | Comp: 0.1846\n",
            "               Val   Loss: 0.3171 | Acc: 0.2459 | Comp: 0.2022\n",
            "No improvement. Patience: 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/100: 100%|██████████| 20/20 [00:06<00:00,  3.17it/s, acc=0.117, comp=0.184, loss=0.106]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 16] Train Loss: 0.1605 | Acc: 0.1548 | Comp: 0.1879\n",
            "               Val   Loss: 0.4150 | Acc: 0.2331 | Comp: 0.2528\n",
            "No improvement. Patience: 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/100: 100%|██████████| 20/20 [00:06<00:00,  3.30it/s, acc=0.11, comp=0.167, loss=0.127]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 17] Train Loss: 0.1424 | Acc: 0.1477 | Comp: 0.1834\n",
            "               Val   Loss: 0.3522 | Acc: 0.2164 | Comp: 0.2328\n",
            "No improvement. Patience: 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/100: 100%|██████████| 20/20 [00:06<00:00,  3.11it/s, acc=0.102, comp=0.136, loss=0.0932]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 18] Train Loss: 0.1210 | Acc: 0.1418 | Comp: 0.1737\n",
            "               Val   Loss: 0.3559 | Acc: 0.2216 | Comp: 0.2307\n",
            "No improvement. Patience: 4/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/100: 100%|██████████| 20/20 [00:06<00:00,  3.10it/s, acc=0.105, comp=0.184, loss=0.133]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 19] Train Loss: 0.1138 | Acc: 0.1372 | Comp: 0.1671\n",
            "               Val   Loss: 0.3621 | Acc: 0.2335 | Comp: 0.2245\n",
            "No improvement. Patience: 5/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/100: 100%|██████████| 20/20 [00:06<00:00,  3.10it/s, acc=0.128, comp=0.129, loss=0.102]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 20] Train Loss: 0.1060 | Acc: 0.1381 | Comp: 0.1636\n",
            "               Val   Loss: 0.3712 | Acc: 0.2288 | Comp: 0.2440\n",
            "No improvement. Patience: 6/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/100: 100%|██████████| 20/20 [00:06<00:00,  3.20it/s, acc=0.156, comp=0.16, loss=0.1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 21] Train Loss: 0.0984 | Acc: 0.1284 | Comp: 0.1579\n",
            "               Val   Loss: 0.4062 | Acc: 0.2449 | Comp: 0.2477\n",
            "No improvement. Patience: 7/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/100: 100%|██████████| 20/20 [00:06<00:00,  3.15it/s, acc=0.107, comp=0.108, loss=0.0722]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 22] Train Loss: 0.0963 | Acc: 0.1291 | Comp: 0.1568\n",
            "               Val   Loss: 0.3611 | Acc: 0.2357 | Comp: 0.2326\n",
            "No improvement. Patience: 8/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/100: 100%|██████████| 20/20 [00:06<00:00,  3.12it/s, acc=0.122, comp=0.128, loss=0.0908]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 23] Train Loss: 0.0944 | Acc: 0.1281 | Comp: 0.1549\n",
            "               Val   Loss: 0.3736 | Acc: 0.2405 | Comp: 0.2279\n",
            "No improvement. Patience: 9/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/100: 100%|██████████| 20/20 [00:06<00:00,  3.25it/s, acc=0.143, comp=0.165, loss=0.0982]\n",
            "                                                         "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 24] Train Loss: 0.0915 | Acc: 0.1260 | Comp: 0.1554\n",
            "               Val   Loss: 0.3709 | Acc: 0.2445 | Comp: 0.2339\n",
            "No improvement. Patience: 10/10\n",
            "Early stopping at epoch 24. Best model from epoch 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "make test folder and testing"
      ],
      "metadata": {
        "id": "UiLwCXlx6lth"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "hcvxWfAyzdbN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b482506-9e99-4b9c-8de3-d1df8273fc38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Test:   7%|▋         | 1/14 [00:02<00:31,  2.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chess03 acc: 0.1418, comp: 0.0880\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest:  14%|█▍        | 2/14 [00:02<00:15,  1.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fire03 acc: 0.1536, comp: 0.0679\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest:  21%|██▏       | 3/14 [00:03<00:09,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "heads01 acc: 0.2650, comp: 0.1464\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest:  29%|██▊       | 4/14 [00:03<00:07,  1.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "office02 acc: 0.1384, comp: 0.1264\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest:  36%|███▌      | 5/14 [00:04<00:05,  1.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "office06 acc: 0.1887, comp: 0.1486\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest:  43%|████▎     | 6/14 [00:04<00:04,  1.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "office07 acc: 0.0979, comp: 0.1478\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest:  50%|█████     | 7/14 [00:05<00:03,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "office09 acc: 0.1249, comp: 0.1261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest:  57%|█████▋    | 8/14 [00:05<00:03,  1.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pumpkin01 acc: 0.1498, comp: 0.1660\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest:  64%|██████▍   | 9/14 [00:06<00:02,  2.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "redkitchen03 acc: 0.2582, comp: 0.1492\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest:  71%|███████▏  | 10/14 [00:06<00:01,  2.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "redkitchen04 acc: 0.2348, comp: 0.1624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest:  79%|███████▊  | 11/14 [00:07<00:01,  2.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "redkitchen06 acc: 0.1651, comp: 0.1427\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest:  86%|████████▌ | 12/14 [00:07<00:00,  2.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "redkitchen12 acc: 0.1848, comp: 0.1313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTest:  93%|█████████▎| 13/14 [00:07<00:00,  2.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "redkitchen14 acc: 0.1796, comp: 0.1412\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                     "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stairs01 acc: 0.1677, comp: 0.1937\n",
            "[Test] Loss: 0.1769 | Acc: 0.1750 | Comp: 0.1384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "os.makedirs(TEST_DIR, exist_ok=True)\n",
        "test(device) #test and save"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "bonus"
      ],
      "metadata": {
        "id": "av-pz6JH6fqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "sparse_sequences = {\n",
        "    \"chess\": \"sparse-seq-05\",\n",
        "    \"fire\": \"sparse-seq-04\",\n",
        "    \"pumpkin\": \"sparse-seq-07\",\n",
        "    \"stairs\": \"sparse-seq-04\"\n",
        "}\n",
        "\n",
        "os.makedirs(BONUS_DIR, exist_ok=True)\n",
        "\n",
        "sparse_samples = []\n",
        "for scene, seq_folder in sparse_sequences.items():\n",
        "    sparse_path = os.path.join(SCENES_ROOT, scene, \"test\", seq_folder)\n",
        "    image_paths = get_keyframe_paths(sparse_path, kf_every=1)\n",
        "    if len(image_paths) == 0:\n",
        "        continue\n",
        "    sparse_samples.append({\n",
        "        \"scene\": scene,\n",
        "        \"sequence\": seq_folder,\n",
        "        \"image_paths\": image_paths[:10],  # 最多 10 張\n",
        "        \"ply_name\": f\"{scene}-{seq_folder}.ply\"\n",
        "    })\n",
        "\n",
        "\n",
        "class SparseBonusDataset(Dataset):\n",
        "    def __init__(self, samples, size=384):\n",
        "        self.samples = samples\n",
        "        self.size = size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        image_paths = sample[\"image_paths\"]\n",
        "        depth_paths = [p.replace(\"color.png\", \"depth.proj.png\") for p in image_paths]\n",
        "        depthmaps, valid_masks = [], []\n",
        "\n",
        "        for path in depth_paths:\n",
        "            depth, mask = load_depth_cv2(path, self.size)\n",
        "            depthmaps.append(torch.from_numpy(depth[None]))\n",
        "            valid_masks.append(torch.from_numpy(mask[None]))\n",
        "\n",
        "        depths = torch.stack(depthmaps)\n",
        "        masks = torch.stack(valid_masks)\n",
        "        images = load_images(image_paths, size=self.size, verbose=False)\n",
        "        images = torch.stack([img_dict[\"img\"].squeeze(0) for img_dict in images])\n",
        "        return {\n",
        "            \"images\": images,\n",
        "            \"depths\": depths,\n",
        "            \"masks\": masks,\n",
        "            \"image_paths\": image_paths,\n",
        "            \"ply_name\": sample[\"ply_name\"]\n",
        "        }\n"
      ],
      "metadata": {
        "id": "5ROSfANmA8p_"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RGBDPointPredictor().to(device)\n",
        "model.load_state_dict(torch.load(\"./checkpoints/best_model.pth\"))\n",
        "model.eval()\n",
        "\n",
        "sparse_loader = DataLoader(SparseBonusDataset(sparse_samples), batch_size=1, shuffle=False)\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(sparse_loader, desc=\"Predicting Bonus\"):\n",
        "        W, H = batch[\"images\"].shape[-1], batch[\"images\"].shape[-2]\n",
        "        images = batch[\"images\"].view(-1, 3, H, W).to(device)\n",
        "        depths = batch[\"depths\"].view(-1, 1, H, W).to(device)\n",
        "        ply_name = batch[\"ply_name\"][0]\n",
        "\n",
        "        pred = model(images, depths)\n",
        "        xyz = pred[:, :3].permute(0, 2, 3, 1).reshape(-1, 3)\n",
        "        xyz_sampled = random_sampling(xyz, 8192)\n",
        "\n",
        "        ply_path = os.path.join(pointcloud_root_bonus, ply_name)\n",
        "        gt_pcd = o3d.io.read_point_cloud(ply_path)\n",
        "        gt_points = np.asarray(gt_pcd.points).astype(np.float32)  # (N, 3)\n",
        "\n",
        "        target_sampled = torch.from_numpy(random_sampling(gt_points, 20000)).to(device)\n",
        "\n",
        "        loss = chamfer_distance(xyz_sampled, target_sampled)\n",
        "        acc = point_cloud_accuracy(xyz_sampled.detach().cpu().numpy(), target_sampled.detach().cpu().numpy())\n",
        "        comp = point_cloud_completeness(xyz_sampled.detach().cpu().numpy(), target_sampled.detach().cpu().numpy())\n",
        "        print(f\"{ply_name} acc: {acc:.4f}, comp: {comp:.4f}\")\n",
        "\n",
        "        save_path = os.path.join(BONUS_DIR, ply_name)\n",
        "\n",
        "        pcd = o3d.geometry.PointCloud()\n",
        "        pcd.points = o3d.utility.Vector3dVector(xyz_sampled.detach().cpu().numpy())\n",
        "        o3d.io.write_point_cloud(save_path, pcd)\n",
        "\n",
        "        print(f\"✅ Saved: {save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MO09QSUBB-O",
        "outputId": "fb001e1d-7aa6-4af8-ce5c-4c2ba2b1e7b9"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Predicting Bonus:  25%|██▌       | 1/4 [00:00<00:01,  1.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chess-sparse-seq-05.ply acc: 0.1606, comp: 0.0826\n",
            "✅ Saved: ./bonus/chess-sparse-seq-05.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rPredicting Bonus:  50%|█████     | 2/4 [00:01<00:01,  1.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fire-sparse-seq-04.ply acc: 0.1898, comp: 0.0717\n",
            "✅ Saved: ./bonus/fire-sparse-seq-04.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rPredicting Bonus:  75%|███████▌  | 3/4 [00:01<00:00,  1.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pumpkin-sparse-seq-07.ply acc: 0.2318, comp: 0.1745\n",
            "✅ Saved: ./bonus/pumpkin-sparse-seq-07.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting Bonus: 100%|██████████| 4/4 [00:02<00:00,  1.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stairs-sparse-seq-04.ply acc: 0.1365, comp: 0.1016\n",
            "✅ Saved: ./bonus/stairs-sparse-seq-04.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "save output(test.zip and bonus.zip)"
      ],
      "metadata": {
        "id": "VfQ_EG-i6Ps2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "!zip -r Result.zip bonus test\n",
        "\n",
        "files.download(\"Result.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "X8aeT3f9lH77",
        "outputId": "f26aacab-30a6-4297-f2c9-52eb5235433f"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: bonus/ (stored 0%)\n",
            "  adding: bonus/chess-sparse-seq-05.ply (deflated 45%)\n",
            "  adding: bonus/stairs-sparse-seq-04.ply (deflated 46%)\n",
            "  adding: bonus/pumpkin-sparse-seq-07.ply (deflated 46%)\n",
            "  adding: bonus/fire-sparse-seq-04.ply (deflated 45%)\n",
            "  adding: test/ (stored 0%)\n",
            "  adding: test/stairs-seq-01.ply (deflated 46%)\n",
            "  adding: test/chess-seq-03.ply (deflated 45%)\n",
            "  adding: test/office-seq-02.ply (deflated 45%)\n",
            "  adding: test/redkitchen-seq-04.ply (deflated 45%)\n",
            "  adding: test/redkitchen-seq-03.ply (deflated 45%)\n",
            "  adding: test/redkitchen-seq-14.ply (deflated 46%)\n",
            "  adding: test/office-seq-09.ply (deflated 45%)\n",
            "  adding: test/heads-seq-01.ply (deflated 45%)\n",
            "  adding: test/redkitchen-seq-12.ply (deflated 45%)\n",
            "  adding: test/office-seq-07.ply (deflated 46%)\n",
            "  adding: test/fire-seq-03.ply (deflated 45%)\n",
            "  adding: test/redkitchen-seq-06.ply (deflated 46%)\n",
            "  adding: test/pumpkin-seq-01.ply (deflated 46%)\n",
            "  adding: test/office-seq-06.ply (deflated 45%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6b621b87-dc9c-4f36-9e79-62da14f2f860\", \"Result.zip\", 1935072)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#clear VRAM\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "uNDnAUgtm-Z-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}